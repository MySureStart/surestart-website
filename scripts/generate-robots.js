#!/usr/bin/env node
/**
 * Robots.txt Generator
 * 
 * Generates environment-aware robots.txt file
 * 
 * Behavior:
 * - Production: Allow crawling + include sitemap
 * - Non-production: Disallow all crawling
 * 
 * Usage:
 *   node scripts/generate-robots.js
 */

const fs = require('fs');
const path = require('path');

// Import SEO config helper
const seoConfig = require('../seo/lib/config');

// Paths
const DIST_DIR = path.resolve(__dirname, '../dist');
const ROBOTS_PATH = path.join(DIST_DIR, 'robots.txt');

/**
 * Generate robots.txt content for production
 */
function generateProductionRobots(baseUrl) {
  return `# Robots.txt - Production
# Generated by SureStart build system

User-agent: *
Allow: /

# Disallow admin and internal paths
Disallow: /admin/
Disallow: /partials/

# Sitemap
Sitemap: ${baseUrl}/sitemap.xml
`;
}

/**
 * Generate robots.txt content for non-production environments
 */
function generateNonProductionRobots(env) {
  return `# Robots.txt - Non-Production (${env})
# Generated by SureStart build system
#
# WARNING: This environment should NOT be indexed by search engines.
# All crawling is disallowed.

User-agent: *
Disallow: /
`;
}

/**
 * Main function
 */
function generateRobots() {
  console.log('ü§ñ Generating robots.txt...\n');
  
  // Check if dist directory exists
  if (!fs.existsSync(DIST_DIR)) {
    console.error('‚ùå Error: /dist directory not found. Run "npm run build" first.');
    process.exit(1);
  }
  
  // Get environment and configuration
  const env = seoConfig.getEnv();
  const isProd = seoConfig.isProd();
  const baseUrl = seoConfig.getBaseUrl().replace(/\/$/, ''); // Remove trailing slash
  
  console.log(`Environment: ${env}`);
  console.log(`Is Production: ${isProd}`);
  console.log(`Base URL: ${baseUrl}\n`);
  
  // Generate appropriate robots.txt content
  let robotsContent;
  
  if (isProd) {
    robotsContent = generateProductionRobots(baseUrl);
    console.log('‚úì Generated PRODUCTION robots.txt');
    console.log('  - Crawling: ALLOWED');
    console.log(`  - Sitemap: ${baseUrl}/sitemap.xml`);
  } else {
    robotsContent = generateNonProductionRobots(env);
    console.log(`‚úì Generated NON-PRODUCTION robots.txt (${env})`);
    console.log('  - Crawling: DISALLOWED');
    console.log('  - This prevents search engines from indexing staging/dev sites');
  }
  
  // Write robots.txt
  fs.writeFileSync(ROBOTS_PATH, robotsContent, 'utf8');
  
  console.log(`\n‚úÖ Robots.txt generated: /dist/robots.txt\n`);
}

// Run
generateRobots();
